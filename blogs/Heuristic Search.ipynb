{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Solving and Search\n",
    "\n",
    "## Backgrounds: \n",
    "\n",
    "Modern research regards problem solving as the Infomation-Processsing approach. It was first proposed by Alan Newell and Herbert Simon, and they described their “logic theorist” computer program that was designed to simulate human problem solving. In other words, instead of just considering the initial structure of a problem and then the new structure achieved when the problem is solved, Newell and Simon described problem solving as a search that occurs between the posing of the problem and its solution.\n",
    "\n",
    "One of the main contributions of Newell and Simon’s approach to problem solving\n",
    "is that it provided a way to specify the possible pathways from the initial to goal states.\n",
    "\n",
    "**Beyond problem space**\n",
    "\n",
    "In phycology perspective, how a problem is stated can affect its diffculty. This is very similar to the Gestalt psychologists’ idea of restructuring. \n",
    "\n",
    "Thus here, I want to talk about the representation of general graph as nodes in Euclidean space, where intuition can be retrieved efficiently because of the structure and closed-form formulars.\n",
    "\n",
    "**The thoughts here:**\n",
    "\n",
    "1. Representation: how graph problems can be facilitated.\n",
    "    - NRL can reveal the structure, good at further application on ML. \n",
    "    - Euclidean Heuristic can preserve the distance information, it's good for informed search, and it can also play a role as NRL. (Explore its potential.)\n",
    "2. FastMap in Euclidean Heuristic, and directed graph space\n",
    "    - The improvement of speed, which a sacrificy on accuracy and admissibility.\n",
    "    - Which parts in FastMap algorithm result in the loss of precision?\n",
    "    - L1, L2 norm; can we adjust L2 to keep the feature.\n",
    "    - In directed graph, where lose the properties, how can we make up for it.\n",
    "    - Try to connect the two embedding into one space, if not, prove they are valuable on application. (An intuition, max to average is like an elastic space model)\n",
    "3. Differential Heuristic\n",
    "    - The idea of true distances storage is necessary for this model\n",
    "    - How can we use it in a reasonable way, like landmark, goal?\n",
    "    - The improvement of accuracy based on a tiny storage. This can be reasonable from application perspective.\n",
    "4. Experiments that support the first section of representation.\n",
    "    - Directed graph path finding\n",
    "    - State lattice, motion planning\n",
    "    - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Representation Learning\n",
    "\n",
    "Adopting this encoder-decoder view, we organize our discussion of the various node embedding methods\n",
    "along the following four methodological components:\n",
    "\n",
    "1. A pairwise similarity function $s_{\\mathcal{G}} : V \\times V \\to R^+$, defined over the graph $G$. This function measures the similarity between nodes in $G$.\n",
    "2. An encoder function, $ENC$, that generates the node embeddings. This function contains a number of trainable parameters that are optimized during the training phase.\n",
    "3. A decoder function, $DEC$, which reconstructs pairwise **similarity values** from the generated embeddings. This function usually contains no trainable parameters.\n",
    "4. A loss function, $\\mathcal{L}$, which determines how the quality of the pairwise reconstructions is evaluated in order to train the model, i.e., how $DEC(z_i, z_j)$ is compared to the true $s_{\\mathcal{G}}(v_i, v_j)$ values.\n",
    "\n",
    "**Thus the question is how to choose similarity values?**\n",
    "\n",
    "Most most common metrics are adjacent, probability of co-occuring in random walk, These various different similarity functions trade-off between modeling “first-order similarity”, where $s_{\\mathcal{G}}$ directly measures connections between nodes (i.e., $s_{\\mathcal{G}}(v_i, v_j)\\triangleq A_{i,j}$) and modeling “higher-order similarity”, where $s_{\\mathcal{G}}$ corresponds to more general notions of neighborhood overlap (e.g., $s_{\\mathcal{G}}(v_i, v_j)\\triangleq A^2_{i,j}$).\n",
    "\n",
    "Or there are more complex similarity like structure similarity, which reveal more of the structure feature of the graph.\n",
    "\n",
    "However, as we can see, it more likely to handle supervised classification and unsupervised clustering problem, as for regression problem, it lose the popularity. And another very significant factor is the asymmetricity of similarity, especially on directed graph.\n",
    "\n",
    "**Shortest path distance:**\n",
    "\n",
    "The intuitions here:\n",
    "\n",
    "1. In uniform weighted graph, the distance is just the depth of BFS, it cannot be refined into a high order approximation, because it will become meaning less.\n",
    "    - However, on the other sider, using distance has already considered the structure by some means, which is what higer order approximation cares, but not the neighborhood situation.\n",
    "    - In other words, the representaion of this method, fastmap algorithm is tend to belong **Graph Constructed from Non-relational Data**\n",
    "2. And during the NRL, what's matters is the similarity between nodes, which means it only keep the similar nodes stay close after embedding, and it doesn't care what's the meaning of the representation.\n",
    "3. NRL ignore the weight of edges and mainly focus on the structure of graph.\n",
    "\n",
    "For Euclidean heuristic, what we care is the distance, which can be very powerful when used on informed search setting. And here we ignored the structure of the graph.\n",
    "\n",
    "Questions here are: \n",
    "- Whether these two pespective can combine together? \n",
    "- And whether there is a need to do so? \n",
    "- Will it bring improvements on achieving their target individually?\n",
    "- We may need to classifiy graph problems according to their goal, structure or distance?\n",
    "- And how about the asymetric problem on directed graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean space embedding\n",
    "\n",
    "One significent feature of Euclidean space is the distances between nodes obeys triangular inequality. And such features enable some properties when mapping it with general graph.\n",
    "\n",
    "And FastMap algorithm start from imaging the objects existing in a Euclidean space where the distance information keeps, and then try to find out the coordinates of nodes in a specified $K$ dimensional space which preserves as much original information as possible.\n",
    "\n",
    "Thus when this two direction meets, what's going on with the ignored structure information of graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection of theorems and properties on graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any edge $(u, v) \\in E$ is the unique shortest $u$-$v$-path is not a limiting assumption when finding shortest paths**.\n",
    "\n",
    "Since any edge $(u, v)$ that does not satisfy the assumption can be removed from the graph without changing the length of any shortest path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Euclidean heuristic is admissible and consistent if and only if the heuristic is locally admissible**, i.e., $\\forall (i, j) \\in E,  \\|y_i −y_j\\|≤ \\delta(i, j)$ where $E$ is the set of edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transitivity is a common characteristic of undirected and directed graphs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
