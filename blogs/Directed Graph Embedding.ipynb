{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Graph Node Embedding (underway)\n",
    "\n",
    "**Graph embedding:** Given the input of a graph $\\mathcal{G} = (V, E)$, and a predefined dimensionality of the embedding $d (d \\ll |V|)$, the problem of graph embedding is to convert $\\mathcal{G}$ into a $d$-dimensional space, in which the graph property is preserved as much as possible. The graph property can be quantified using proximity measures such as the first- and higher-order proximity. Each graph is represented as either a $d$-dimensional vector (for a whole graph) or a set of $d$-dimensional vectors with each vector representing the embedding of part of the graph (e.g., node, edge, substructure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Euclidean space, the feature used most after embedding is distance, which preserves the original graph property in which we are interested. In the majority situation, such similarity matric is defined based on the graph structure, like neighborhood.\n",
    "\n",
    "In Network Representation Learning (NRL) field, the embedded vector will be used to do machine learning tasks later, and the tasks are mainly classification and clustering. Because it consider more about the structure, thus the first-order proximity is the local pairwise similarity while higher order consider about neighborhoods, and the formular here becomes $s_{ij}^{(k)} = cosine(s_{i}^{(k-1)}, s_{j}^{(k-1)})$.\n",
    "\n",
    "Another perspective is using such embedding as a preprocessing process, which makes the information of the original graph which is preserved by the distances between objects can be easily retrieved later at the running time. In other words, it's much more like solving a regression task. For example, Euclidean Heuristic, which is used in heuristic search, uses the distances of nodes in Euclidean space to approximate the shortest path distances in orignial graph.\n",
    "\n",
    "For both of them, there are two kinds of queries need to be finished efficiently on the embeded Euclidean space:\n",
    "\n",
    "1. Query by example: require getting the embedding of a specific node easily. (i.e. KNN classification)\n",
    "    - For representation learning, it should be able to handle the embedding of a previously unseen new nodes quickly.\n",
    "    - For preprocessing, every nodes will get a pre-calculated vector as a representation. \n",
    "2. All pairs query: based on the close-formed calculation of distances in Euclidean space, this can be done quickly. (i.e. K-means clustering)\n",
    "\n",
    "In Graph Embedding, we emphasis low-dimensional representation, which kind of related to efficient calculation. A detailed survey about it can be find [here](https://arxiv.org/abs/1709.07604)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interlude: FastMap Algorithm and Euclidean Heuristic**\n",
    "\n",
    "1. FastMap works on imput \"Graph Constructed from Non-relational Data\"\n",
    "    - However, for Graph FastMap, we works on homogenerous weighted graph, homogenerous directed weighted graph underway, and stochastic graph (heterogeneous direct weighted graph) in the future.\n",
    "    - The output is node embedding. \n",
    "    - However, is it merely a node embedding? Because what we use in directed graph is asymmetric distances bewteen a pair of nodes, thus it's more like a feature of edge embedding.\n",
    "2. Here we exploit \"semi-high-ordered proximity\", which uses shortest path distance and kind of related to structure of graph but not considering the neighborhood structure that much.\n",
    "3. The global structure of a graph (e.g., paths, tree, subgraph patterns) is omitted in most graph embedding techniques, and in graph FastMap we mainly uses the path information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal in this article**\n",
    "\n",
    "How to solve the asymmetricity of directed graph and embed it into Euclidean space?\n",
    "\n",
    "**Latent question**\n",
    "\n",
    "Can structures rather than distances in Euclidean space show some properties of the original graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type One \n",
    "\n",
    "Some algorithms tries to get a symmetric similarity metric between nodes of directed graph, then the graph can be embedding into Euclidean space while the distances between nodes just shows the symmetric locality property.\n",
    "\n",
    "Representaion algorithms:\n",
    "\n",
    "[**DGE**](http://www.aaai.org/Library/IJCAI/2007/ijcai07-435.php):\n",
    "\n",
    "The goal of this study is solving nodes classification problem on directed graph, so they proposed a method using transition probability together with the stationary distribution of Markov random walks to measure the locality property. The measure they want to minimize while embedding is $(\\pi_u p(u,v)+\\pi_v p(v,u))/2$, where $p(u,v) = w(u,v)/\\sum_{v,u\\to v}w(u,v)$. As we can see it depends on the stationary distribution $\\pi_u$ and the out-link weights proportion.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "This is effective when solving classification and clustering tasks, and there is no need to redesign backend algorithms for specific task from embedded space.\n",
    "\n",
    "However, what if the information we want to preserveis asymmetric per se? Which means $Dis(i,j)\\ne Dis(j,i)$. Then we need a second type of embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Two\n",
    "\n",
    "Using two different embedding. The consideration here is which information what to preserve? What's more, how to put the two learning embedding into one space and how they related to each other? This is what we need in DIrected Graph FastMap (DIG-FastMap).\n",
    "\n",
    "Representaion algorithms:\n",
    "\n",
    "[HOPE](https://dl.acm.org/citation.cfm?id=2939751):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
